{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Getting Started with CUDA in Python\n",
                "\n",
                "This notebook introduces GPU programming using Numba and CuPy on Google Colab.\n",
                "\n",
                "**Before running:** Make sure GPU is enabled!\n",
                "- `Runtime ‚Üí Change runtime type ‚Üí GPU`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Check GPU Availability"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check what GPU we have\n",
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Verify Numba CUDA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from numba import cuda\n",
                "import numpy as np\n",
                "\n",
                "print(\"CUDA Available:\", cuda.is_available())\n",
                "\n",
                "if cuda.is_available():\n",
                "    print(\"GPU Name:\", cuda.get_current_device().name.decode())\n",
                "    print(\"Compute Capability:\", cuda.get_current_device().compute_capability)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Verify CuPy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import cupy as cp\n",
                "\n",
                "print(\"CuPy version:\", cp.__version__)\n",
                "print(\"CUDA version:\", cp.cuda.runtime.runtimeGetVersion())\n",
                "\n",
                "# Simple test\n",
                "x_gpu = cp.array([1, 2, 3, 4, 5])\n",
                "print(\"\\nGPU array:\", x_gpu)\n",
                "print(\"Sum on GPU:\", cp.sum(x_gpu))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ First Numba CUDA Kernel: Vector Addition\n",
                "\n",
                "Let's write our first CUDA kernel to add two arrays element-wise."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from numba import cuda\n",
                "import numpy as np\n",
                "import math\n",
                "\n",
                "@cuda.jit\n",
                "def add_kernel(a, b, c):\n",
                "    \"\"\"\n",
                "    CUDA kernel to add two arrays: c = a + b\n",
                "    Each thread handles one element.\n",
                "    \"\"\"\n",
                "    idx = cuda.grid(1)  # Get global thread ID\n",
                "    \n",
                "    if idx < c.size:  # Boundary check\n",
                "        c[idx] = a[idx] + b[idx]\n",
                "\n",
                "# Create test data\n",
                "n = 1_000_000\n",
                "a = np.ones(n, dtype=np.float32)\n",
                "b = np.ones(n, dtype=np.float32) * 2\n",
                "c = np.zeros(n, dtype=np.float32)\n",
                "\n",
                "# Copy to GPU\n",
                "a_gpu = cuda.to_device(a)\n",
                "b_gpu = cuda.to_device(b)\n",
                "c_gpu = cuda.to_device(c)\n",
                "\n",
                "# Configure kernel launch\n",
                "threads_per_block = 256\n",
                "blocks = math.ceil(n / threads_per_block)\n",
                "\n",
                "# Launch kernel\n",
                "add_kernel[blocks, threads_per_block](a_gpu, b_gpu, c_gpu)\n",
                "\n",
                "# Copy result back to CPU\n",
                "c_result = c_gpu.copy_to_host()\n",
                "\n",
                "print(f\"First 10 results: {c_result[:10]}\")\n",
                "print(f\"All values correct: {np.allclose(c_result, 3.0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ CuPy: NumPy on GPU\n",
                "\n",
                "CuPy provides a NumPy-like interface for GPU arrays."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import cupy as cp\n",
                "import numpy as np\n",
                "import time\n",
                "\n",
                "# Create large arrays\n",
                "n = 10_000_000\n",
                "\n",
                "# CPU version\n",
                "a_cpu = np.random.rand(n)\n",
                "b_cpu = np.random.rand(n)\n",
                "\n",
                "start = time.time()\n",
                "c_cpu = a_cpu + b_cpu\n",
                "cpu_time = time.time() - start\n",
                "\n",
                "# GPU version\n",
                "a_gpu = cp.random.rand(n)\n",
                "b_gpu = cp.random.rand(n)\n",
                "\n",
                "start = time.time()\n",
                "c_gpu = a_gpu + b_gpu\n",
                "cp.cuda.Stream.null.synchronize()  # Wait for GPU to finish\n",
                "gpu_time = time.time() - start\n",
                "\n",
                "print(f\"CPU time: {cpu_time:.4f}s\")\n",
                "print(f\"GPU time: {gpu_time:.4f}s\")\n",
                "print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Matrix Multiplication Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import cupy as cp\n",
                "import numpy as np\n",
                "import time\n",
                "\n",
                "# Matrix size\n",
                "size = 2000\n",
                "\n",
                "# CPU\n",
                "A_cpu = np.random.rand(size, size)\n",
                "B_cpu = np.random.rand(size, size)\n",
                "\n",
                "start = time.time()\n",
                "C_cpu = np.dot(A_cpu, B_cpu)\n",
                "cpu_time = time.time() - start\n",
                "\n",
                "# GPU\n",
                "A_gpu = cp.random.rand(size, size)\n",
                "B_gpu = cp.random.rand(size, size)\n",
                "\n",
                "start = time.time()\n",
                "C_gpu = cp.dot(A_gpu, B_gpu)\n",
                "cp.cuda.Stream.null.synchronize()\n",
                "gpu_time = time.time() - start\n",
                "\n",
                "print(f\"Matrix size: {size}x{size}\")\n",
                "print(f\"CPU time: {cpu_time:.4f}s\")\n",
                "print(f\"GPU time: {gpu_time:.4f}s\")\n",
                "print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéØ Summary\n",
                "\n",
                "In this notebook, you learned:\n",
                "- ‚úÖ How to check GPU availability in Colab\n",
                "- ‚úÖ Verify Numba and CuPy installations\n",
                "- ‚úÖ Write a simple CUDA kernel with Numba\n",
                "- ‚úÖ Use CuPy for NumPy-like GPU operations\n",
                "- ‚úÖ Compare CPU vs GPU performance\n",
                "\n",
                "**Next Steps:**\n",
                "- Learn about CUDA thread hierarchy (blocks, grids, threads)\n",
                "- Explore memory management (global, shared, local memory)\n",
                "- Optimize kernel performance"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}